# Pandasデータフレーム加工の例

- pandasのデータフレームを加工するケースが多いため、その例を示す。
- 具体的な記法については、このあとの章で解説する

## 実施内容
1. `sample.csv`ファイルは下記の構成で、5000行あるとする

    | col1 | col2  | col3  | col4   |
    | ---- | ----- | ----- | ------ |
    | 0    | 10000 | 50000 | 100000 |
    | 1    | 10000 | 50000 | 100000 |
    | 2    | 10000 | 50000 | 100000 |
    | 3    | 10000 | 50000 | 100000 |
    | 4    | 10000 | 50000 | 100000 |

2. あらたらカラム`sum`を追加し、`col1 + col2 + col3 + col4` したものを入れる
   完成形は下記の形
   
    | col1 | col2  | col3  | col4   | sum    |
    | ---- | ----- | ----- | ------ | ------ |
    | 0    | 10000 | 50000 | 100000 | 160000 |
    | 1    | 10000 | 50000 | 100000 | 160001 |
    | 2    | 10000 | 50000 | 100000 | 160002 |
    | 3    | 10000 | 50000 | 100000 | 160003 |
    | 4    | 10000 | 50000 | 100000 | 160004 |

3. 2の処理を並列化なし/並列化あり（マルチプロセス）で計測する
    - CPUバウンド処理のため、マルチプロセスのみとする

## 処理内容
1. `sample.csv`を読み込み、pandasデータフレームへ格納する
2. データフレームをCPUコア数分に分割する
3. 各データフレームを同時に計算する


```python:measurement.py
from contextlib import contextmanager
import multiprocessing
import time

import pandas as pd
import numpy as np

import dask.dataframe as dd


# 時間計測用コンテキストマネージャ。　処理には関係ない
@contextmanager
def timer(name):
    """
    Time関数
    :param 表示文字列:
    :return: 処理時間表示
    """
    t0 = time.time()
    yield
    print(f'[{name}] done in {time.time() - t0} s')


def split_parallel(df, cpu_num, map_func):
    """
    マルチプロセスで、DataFrameを処理するための関数
    :param df: inputするDataFrame
    :param cpu_num: CPU数=マルチプロセス数=DataFrame分割数
    :param map_func: DataFrameの処理内容の実弟
    :return: 各プロセスが処理したDataFrameを結合したもの
    """

    # DataFrameをcpu_num分に分割する
    df_split = np.array_split(df, cpu_num)

    # map処理で map_funcに分割したDFのリストを渡す
    with multiprocessing.Pool(cpu_num) as p:
        result = p.map(map_func, df_split)

    # 各DataFrameを結合する
    return pd.concat(result)


def map_count(df):
    """
    DataFrameに対して、処理したい内容
    :param df: input DataFrame
    :return: 処理したDataFrame
    """

    df['sum'] = df['col1'] + df['col2'] + df['col3'] + df['col4']
    return df


def main():
    # CPU数の取得
    cpu_num = multiprocessing.cpu_count()
    print("Number of cpu : ", cpu_num)

    # FileRead
    file_name = './sample.csv'
    input_df = pd.read_csv(file_name)

    # ----------------------- SingleProcessでの処理時間 -----------------------
    with timer('SingleProcess Time'):
        input_df.apply(map_count, axis=1)

        # input_df= input_df.apply(map_count, axis=1)
    # print(input_df)

    # ----------------------- MultiProcessでの処理時間 -----------------------
    with timer('MultiProcess Time'):
        split_parallel(input_df, cpu_num, map_count)

        # mp_df = split_parallel(input_df, cpu_num, map_count)
    # print(mp_df)
    

if __name__ == '__main__':
    main()
```


```sh:実施結果
Number of cpu :  8
[SingleProcess Time] done in 5.066002130508423 s
[MultiProcess Time] done in 2.306995391845703 s
```


### 【参考】Dask
- `Dask`とはPandasやNumPyの並列処理を簡単に行うためのライブラリ
- 対象は`メモリに乗り切らない巨大なデータセット`に対し、Pandas,Numpy処理をする
- 複数台のPCでの動作も想定している。
- 扱えるデータ量は Pandas/Numpy < Dask < Sparkとなる

#### Dask利用方法
- 前項のソースコードに加え、Dask処理を追加する
```python
    import dask.dataframe as dd
    # ----------------------- Daskでの処理時間 -----------------------
    # メモリ内に収まるデータであれば、Daskを使うメリットはあまりない。

    # DFをDask_DFに変換(npartitions:分割数)
    dask_df = dd.from_pandas(input_df, npartitions=cpu_num)

    # Dask処理用のメタデータ定義
    meta = {
        'col1': 'int64',
        'col2': 'int64',
        'col3': 'int64',
        'col4': 'int64',
        'sum': 'int64'}

    # Daskでの処理時間計測
    with timer('DaskProcess Time'):
        dask_df.apply(map_count, axis=1, meta=meta).compute()

        # dask_df = dask_df.apply(map_count, axis=1, meta=meta).compute()
    # print(dask_df)
```

```sh:実行結果
Number of cpu :  8
[SingleProcess Time] done in 5.066002130508423 s
[MultiProcess Time] done in 2.306995391845703 s
[DaskProcess Time] done in 5.186000823974609 s
```
- 今回はデータセットが小さいため、効果はない。（むしろ悪化）
- Daskの裏ではmultiprocessing、threadingが動作している